{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, metrics, svm, ensemble\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd, numpy, string\n",
    "import seaborn as pl\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "#Remove Special Charactors\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (31962, 3) 31962\n",
      "Test Set: (17197, 2) 17197\n"
     ]
    }
   ],
   "source": [
    "#Import Training and Testing Data\n",
    "train = pd.read_csv('train.csv')\n",
    "print(\"Training Set:\"% train.columns, train.shape, len(train))\n",
    "test = pd.read_csv('test_tweets.csv')\n",
    "print(\"Test Set:\"% test.columns, test.shape, len(test))\n",
    "#Tokenize words in order to clean and stem\n",
    "tok = WordPunctTokenizer()\n",
    "# patterns to remove html tags numbers and special Characters\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "porter=PorterStemmer()\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # Tokenize and join together to remove unnecessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    #Stemming\n",
    "    stem_sentence=[]\n",
    "    for word in words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    #Rejoin the words back to create the cleaned tweet\n",
    "    words=\"\".join(stem_sentence).strip()    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayaan\\AppData\\Local\\Temp\\ipykernel_11276\\1957111177.py:14: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "nums = [0,len(train)]\n",
    "clean_tweet_texts = []\n",
    "for i in range(nums[0],nums[1]):\n",
    "    clean_tweet_texts.append(tweet_cleaner(train['tweet'][i]))\n",
    "nums = [0,len(test)]\n",
    "test_tweet_texts = []\n",
    "for i in range(nums[0],nums[1]):\n",
    "    test_tweet_texts.append(tweet_cleaner(test['tweet'][i]))\n",
    "#Dataframes to represent the testing and training samples\n",
    "train_clean = pd.DataFrame(clean_tweet_texts,columns=['tweet'])\n",
    "train_clean['label'] = train.label\n",
    "train_clean['id'] = train.id\n",
    "test_clean = pd.DataFrame(test_tweet_texts,columns=['tweet'])\n",
    "test_clean['id'] = test.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_clean['tweet'],train_clean['label'])\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words='english', max_features=100000)\n",
    "tfidf_vect.fit(train_clean['tweet'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(train_clean['tweet'])\n",
    "xtrain_bow =  tfidf_vect.transform(train_x)\n",
    "xvalid_bow =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the f1 Score\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    return metrics.f1_score(valid_y,predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return Prediction Labels\n",
    "def train_model_Pred(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF:  0.55819774718398\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"SVM, WordLevel TF-IDF: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_clean['tweet']\n",
    "valid_x=test_clean['tweet']\n",
    "train_y=train_clean['label']\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words='english', max_features=100000)\n",
    "tfidf_vect.fit(train_clean['tweet'])\n",
    "tfidf_vect.fit(test_clean['tweet'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF:  [0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model_Pred(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"SVM, WordLevel TF-IDF: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to check your results manually you may include the tweet:d={'id':test['id'],'Tweet':valid_x,'label':accuracy}\n",
    "d={'id':test['id'],'label':accuracy}\n",
    "df=pd.DataFrame(data=d)\n",
    "df.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13ce007b055d928af5890303867c4c5fac283220f021e659ba8fb53d4ac2ffc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
